1
00:00:00,290 --> 00:00:03,230
Okay, welcome back, discrete optimization,
knapsack.

2
00:00:03,230 --> 00:00:04,750
So I want to go back to the branch and

3
00:00:04,750 --> 00:00:07,340
bound, and talk a little bit about search
strategies.

4
00:00:07,340 --> 00:00:11,160
I put something under the rug, and I
want to cover them during this lecture.

5
00:00:11,160 --> 00:00:12,530
And you're going to see, some of these
things

6
00:00:12,530 --> 00:00:14,650
are going to be very interesting later on,
okay?

7
00:00:14,650 --> 00:00:16,270
But you need, you need to understand them.

8
00:00:16,270 --> 00:00:19,570
It's going to build on branch and bound,
but just refine the kinds of thing,

9
00:00:19,570 --> 00:00:23,250
actually expand the kinds of things that
you can do in branch and bound.

10
00:00:23,250 --> 00:00:25,700
Okay?
Remember, branch and one we are working

11
00:00:25,700 --> 00:00:27,730
with real artifacts that we can break but

12
00:00:27,730 --> 00:00:30,520
still we are using the linear relaxation
in general,

13
00:00:30,520 --> 00:00:34,530
or relaxation in general to actually
compute good, you

14
00:00:34,530 --> 00:00:37,000
know, optimistic episodes of the best you
can do.

15
00:00:37,000 --> 00:00:37,610
Okay?

16
00:00:37,610 --> 00:00:40,450
So once again the key idea between branch
and bound is that you look at

17
00:00:40,450 --> 00:00:46,190
this exhaustive search, and you want to
explore a very tiny piece of this tree.

18
00:00:46,190 --> 00:00:46,840
Okay?

19
00:00:46,840 --> 00:00:49,960
And you do that with basic, you know,
relaxation ID.

20
00:00:49,960 --> 00:00:50,560
Okay?

21
00:00:50,560 --> 00:00:50,860
Now one

22
00:00:50,860 --> 00:00:53,010
of the things I didn't talk about last
time is

23
00:00:53,010 --> 00:00:56,114
that you can search that tree in many
different ways.

24
00:00:56,114 --> 00:00:58,750
So, we saw that first branch and bound and
I will talk about a little bit

25
00:00:58,750 --> 00:01:01,310
about it again such that to refresh your

26
00:01:01,310 --> 00:01:03,380
memory, but there are other things you can
do.

27
00:01:03,380 --> 00:01:07,130
And some of them are, you know, code best,
you know, best, best search, best-first

28
00:01:07,130 --> 00:01:09,380
search or, you know, limited discrepancy
search,

29
00:01:09,380 --> 00:01:11,079
and that's what I want to cover today.

30
00:01:11,079 --> 00:01:13,147
Okay I want to give you an idea that, you
know, there

31
00:01:13,147 --> 00:01:15,710
is one thing that you need to consider
when you solve a problem.

32
00:01:15,710 --> 00:01:16,110
It's also

33
00:01:16,110 --> 00:01:19,740
how you explore that, that particular,
that particular trait.

34
00:01:19,740 --> 00:01:22,340
There are many others, and you will see
more in this class, okay?

35
00:01:22,340 --> 00:01:24,090
But at this point, I just want to give you
some of

36
00:01:24,090 --> 00:01:26,630
the ideas on what you can do, and compare
them, okay?

37
00:01:26,630 --> 00:01:28,260
So that first search, you explore your,
your,

38
00:01:28,260 --> 00:01:29,650
your, no, that's what we saw last time.

39
00:01:29,650 --> 00:01:31,876
You explore your tree in a tree in that
first fashion,

40
00:01:31,876 --> 00:01:34,450
and you prune the search tree as soon as
you come

41
00:01:34,450 --> 00:01:37,190
back to a node, and that node is an
optimistic evaluation

42
00:01:37,190 --> 00:01:40,110
which is worse than the best solutions you
have found, okay?

43
00:01:40,110 --> 00:01:41,210
That's depth-first search,

44
00:01:41,210 --> 00:01:43,040
okay?
Now we going to see best-first search.

45
00:01:43,040 --> 00:01:45,760
And the key idea there is that you want to

46
00:01:45,760 --> 00:01:49,330
select at any stage the node which has the
best evaluation.

47
00:01:49,330 --> 00:01:50,260
Okay.

48
00:01:50,260 --> 00:01:52,120
So that's what best-first is.

49
00:01:52,120 --> 00:01:54,820
And we'll see a limited discrepency search
where

50
00:01:54,820 --> 00:01:57,530
what you will do is trusting a greedy
heuristic.

51
00:01:57,530 --> 00:02:01,150
And you'll be, be following these
heuristic and then allowing you

52
00:02:01,150 --> 00:02:04,630
to move away from the heuristic in a very
systematic fashion.

53
00:02:04,630 --> 00:02:07,210
So I'll go into the details of these two
things, okay?

54
00:02:07,210 --> 00:02:12,030
So let, you know I just want to remind you
of that first search first, okay?

55
00:02:12,030 --> 00:02:13,800
And we are using the good eval-, the

56
00:02:13,800 --> 00:02:16,680
linear relaxation here for the knapsack
problem, okay?

57
00:02:16,680 --> 00:02:19,740
And remember what we did was basically
exploring these tree, we

58
00:02:19,740 --> 00:02:21,670
would find a good solution there, with a
cost of 80.

59
00:02:21,670 --> 00:02:24,120
And then we would go back to the other
part

60
00:02:24,120 --> 00:02:26,520
of the tree here, where we're done selling
the first item.

61
00:02:26,520 --> 00:02:30,080
And you see that the optimistic evaluation
here is worse than

62
00:02:30,080 --> 00:02:32,520
the best solution that we have found, and
therefore you can

63
00:02:32,520 --> 00:02:33,960
prove this entire tree.

64
00:02:33,960 --> 00:02:36,350
Okay so that's what I told you before, in
that first

65
00:02:36,350 --> 00:02:40,360
search, when you have an optimistic
evaluation of a note, you compare

66
00:02:40,360 --> 00:02:42,610
it to the best solutions that you have
found so far,

67
00:02:42,610 --> 00:02:46,240
and if it's worse then you just throw the
entire sub-tree away.

68
00:02:46,240 --> 00:02:48,750
Okay?
So that's what that first search is about.

69
00:02:48,750 --> 00:02:49,400
Okay?

70
00:02:49,400 --> 00:02:51,750
So in a sense, you know, the key idea for
that

71
00:02:51,750 --> 00:02:54,340
search is like in the NFL, you want to go
deep.

72
00:02:54,340 --> 00:02:55,080
Okay?

73
00:02:55,080 --> 00:02:58,240
And, and, and try to find good solutions
quickly.

74
00:02:58,240 --> 00:03:00,330
Okay, when does it prune?

75
00:03:00,330 --> 00:03:03,600
It prunes as soon as you're reach a note

76
00:03:03,600 --> 00:03:06,420
which who's evaluation is worth in the
best found solution.

77
00:03:06,420 --> 00:03:06,670
Okay?

78
00:03:06,670 --> 00:03:08,670
And one of the questions I have for you

79
00:03:08,670 --> 00:03:12,990
is, how efficient is it from a memory
standpoint, okay?

80
00:03:12,990 --> 00:03:17,710
And I'm going to always ask you questions
like this during this class, okay?

81
00:03:17,710 --> 00:03:21,350
And the key anyone you want to answer them
is, you have to exaggerate, okay?

82
00:03:21,350 --> 00:03:23,390
So, just assume, you know,

83
00:03:23,390 --> 00:03:27,420
extreme cases and try to see all these
things are working, okay?

84
00:03:27,420 --> 00:03:29,920
So, how much space, you know, is a branch

85
00:03:29,920 --> 00:03:32,540
and bound, is that first branch and bound
going to take?

86
00:03:32,540 --> 00:03:36,130
Okay, can you find, can you, can you find
the answer to that?

87
00:03:36,130 --> 00:03:36,998
Well it's very simple, right?

88
00:03:36,998 --> 00:03:37,340
.

89
00:03:37,340 --> 00:03:40,550
You always go left, left, left until you
get stuck.

90
00:03:40,550 --> 00:03:41,200
Okay?

91
00:03:41,200 --> 00:03:44,630
So essentially the number of notes you can
accumulate in memory

92
00:03:44,630 --> 00:03:48,790
of one time is essentially the length of
that particular branch,

93
00:03:48,790 --> 00:03:52,830
which is this particular case is the
number of items you can put in an upside.

94
00:03:52,830 --> 00:03:55,500
So yes, it's actually pretty memory
efficient because

95
00:03:55,500 --> 00:03:57,620
you look at everyone of the item, you

96
00:03:57,620 --> 00:04:00,490
have to do that because you have to find
out whether you select them or not.

97
00:04:00,490 --> 00:04:02,970
And that's what you select inside a
particular branch.

98
00:04:02,970 --> 00:04:04,950
Once you have, you know, explored that
branch you

99
00:04:04,950 --> 00:04:06,560
can remove some of these notes, and you
would

100
00:04:06,560 --> 00:04:09,530
select other ones by sticking the other
branches, but

101
00:04:09,530 --> 00:04:12,414
you always have essentially one branch at
any one time.

102
00:04:13,650 --> 00:04:14,340
Okay?

103
00:04:14,340 --> 00:04:17,920
So, lets look at the next technique which
is best-first branch and bound.

104
00:04:17,920 --> 00:04:19,990
And to illustrate this, I'm going to use a

105
00:04:19,990 --> 00:04:22,960
very, very simple relaxation, the one
where we remove

106
00:04:22,960 --> 00:04:25,890
the capacity constraint, because that
allows me to express,

107
00:04:25,890 --> 00:04:28,550
you know, illustrate a concepts in a
better fashion.

108
00:04:28,550 --> 00:04:32,990
Same, you know, same ratio of, you know,
values weight that done before.

109
00:04:32,990 --> 00:04:34,900
Same capacity for nap sack.
Okay?

110
00:04:34,900 --> 00:04:38,720
And once again all the notes, the values
that I've accumulated, the space left in

111
00:04:38,720 --> 00:04:41,780
the knapsack and the optimistic
evaluation, okay.

112
00:04:41,780 --> 00:04:43,510
So, initially best research, look at the

113
00:04:43,510 --> 00:04:46,375
particular item, and then generate two
sub-problems, okay.

114
00:04:46,375 --> 00:04:49,600
Whether you select the item or you don't
select the item, okay.

115
00:04:49,600 --> 00:04:52,030
And now that first search, or best-first
search, is

116
00:04:52,030 --> 00:04:54,420
going to look at all the notes which are
there, okay.

117
00:04:54,420 --> 00:04:56,670
And select the one, okay, which is not
close.

118
00:04:56,670 --> 00:04:59,520
Which means we haven't explored, you know,
its children yet.

119
00:04:59,520 --> 00:05:01,590
Okay, which is the best evaluation?

120
00:05:01,590 --> 00:05:03,890
And in this particular case, we take the
one where we selected

121
00:05:03,890 --> 00:05:06,800
the item, and which has a value of 128,
right?

122
00:05:06,800 --> 00:05:10,300
And we basically explore its, you know,
generate its two children, okay?

123
00:05:10,300 --> 00:05:14,050
One of them is infeasible, and the other
one is a value 80, okay?

124
00:05:14,050 --> 00:05:15,440
And now we basically have to consider

125
00:05:15,440 --> 00:05:17,880
these three notes, one of which is
infeasible.

126
00:05:17,880 --> 00:05:19,880
So we have essentially to consider these
two.

127
00:05:19,880 --> 00:05:22,720
And we take the one which has the best
evaluation, which is this one.

128
00:05:22,720 --> 00:05:26,005
Okay, we expand the children again, okay,
and what

129
00:05:26,005 --> 00:05:29,071
we get now is these three notes, okay,
which have

130
00:05:29,071 --> 00:05:31,387
evaluated tree and, and then 35.

131
00:05:31,387 --> 00:05:33,339
And once again we take the one which has

132
00:05:33,339 --> 00:05:36,222
the best evaluation, 83, okay, and we
expand it.

133
00:05:36,222 --> 00:05:38,616
And we get a feasible solution there which
has

134
00:05:38,616 --> 00:05:41,950
a pretty bad value of 48, and an
infeasible solution.

135
00:05:41,950 --> 00:05:46,230
Okay, so at that point once again, you
know, we will be basically evaluating,

136
00:05:46,230 --> 00:05:50,770
taking the best note, the note which has
the best evaluation for expansion, okay?

137
00:05:50,770 --> 00:05:52,740
So it's going to, so this note we already
know,

138
00:05:52,740 --> 00:05:53,940
but I'll come back to that in a moment.

139
00:05:53,940 --> 00:05:54,320
We already

140
00:05:54,320 --> 00:05:56,340
know that we would never select it right?

141
00:05:56,340 --> 00:05:58,490
Because it's worth in the best solution
that we have,

142
00:05:58,490 --> 00:06:01,140
but we expand this one and we find two
solutions.

143
00:06:01,140 --> 00:06:03,920
One with value 80 and one with value 45.

144
00:06:03,920 --> 00:06:06,510
Okay, and this is the best solution that
we have.

145
00:06:06,510 --> 00:06:08,611
At this point we look at the best, you

146
00:06:08,611 --> 00:06:11,510
know, note, which is not expanded inside a
tree.

147
00:06:11,510 --> 00:06:15,290
We see this guy, but you see this guy
doesn't have to be expanded, which is

148
00:06:15,290 --> 00:06:19,370
already worse than the best solution we
have, we have found so far, so we can stop

149
00:06:19,370 --> 00:06:22,010
the exploration at that point.
Okay?

150
00:06:22,010 --> 00:06:23,680
So that's best for search.
Okay.

151
00:06:23,680 --> 00:06:25,650
You always look at the note which has the

152
00:06:25,650 --> 00:06:28,590
best optimistic evaluation that is the one
that you select.

153
00:06:28,590 --> 00:06:32,020
You expand, you know its children, and
then you repeat the process

154
00:06:32,020 --> 00:06:35,730
select the one which is the best
evaluation and keep doing this, okay?

155
00:06:35,730 --> 00:06:39,000
So, in a sense best-first search is always
greedy, right,

156
00:06:39,000 --> 00:06:41,930
always go for the best, okay, and expand
that one, okay?

157
00:06:41,930 --> 00:06:44,280
Most of the time when you expand that
particular note,

158
00:06:44,280 --> 00:06:47,430
you know, they will, their value may go up
and therefore you

159
00:06:47,430 --> 00:06:50,638
may go down and therefore you don't select
that many more, okay?

160
00:06:50,638 --> 00:06:52,780
When is best-first search pruning?

161
00:06:52,780 --> 00:06:55,510
It's essentially pruning when, you know,
you

162
00:06:55,510 --> 00:06:57,660
tried all the nodes that you can expand

163
00:06:57,660 --> 00:07:00,930
if a value which is worst than the best
solutions that you have found so far.

164
00:07:00,930 --> 00:07:03,440
At that point, you have all these nodes
which are floating around.

165
00:07:03,440 --> 00:07:06,370
They've not been expanded, okay, but it
makes no

166
00:07:06,370 --> 00:07:08,710
sense to actually explore any of them, so
you stop.

167
00:07:08,710 --> 00:07:09,250
And that's,

168
00:07:09,250 --> 00:07:13,410
you know, when the computation stops.
Now, is it memory efficient?

169
00:07:13,410 --> 00:07:15,090
Think about it, okay, once again.

170
00:07:15,090 --> 00:07:16,620
Exaggerate.
Okay?

171
00:07:16,620 --> 00:07:19,740
How would we be exaggerating in this
particular problem?

172
00:07:19,740 --> 00:07:21,760
Okay?
Well we can be just city.

173
00:07:21,760 --> 00:07:24,640
Right, suppose that, you know, the, the

174
00:07:24,640 --> 00:07:27,860
optimistic evaluation is going to be
amazingly optimistic.

175
00:07:27,860 --> 00:07:30,342
It's going to give you plus infinity all
the time.

176
00:07:30,342 --> 00:07:32,070
So if you do that, how much space

177
00:07:32,070 --> 00:07:35,490
are you going to use in best-first branch
and bound?

178
00:07:35,490 --> 00:07:36,820
Think about it, okay?

179
00:07:36,820 --> 00:07:39,200
Essentially what you going to do is, take
an oath.

180
00:07:39,200 --> 00:07:41,380
They have all the same value, infinity.

181
00:07:41,380 --> 00:07:42,780
You're going to expand one, okay?

182
00:07:42,780 --> 00:07:44,610
And then you're going to expand the other
one, and so on.

183
00:07:44,610 --> 00:07:46,320
And you will have to expand all of them.

184
00:07:46,320 --> 00:07:49,190
So, you're going to store the entire tree
inside

185
00:07:49,190 --> 00:07:52,060
memory, which is kind of a disaster,
right?

186
00:07:52,060 --> 00:07:54,140
Because normally you will take exponential
time,

187
00:07:54,140 --> 00:07:56,470
but you will also take exponential space.

188
00:07:56,470 --> 00:07:59,620
And space is actually one of the main
limits in computers these days.

189
00:07:59,620 --> 00:08:00,620
Okay, so,

190
00:08:00,620 --> 00:08:02,530
you know, you know this is, this is the

191
00:08:02,530 --> 00:08:06,040
worst case for you know a best-first
branch and bound.

192
00:08:06,040 --> 00:08:10,150
Know you may ask, when is best-first
branch and bound really good?

193
00:08:10,150 --> 00:08:12,910
Okay?
And once again, exaggerate, right?

194
00:08:12,910 --> 00:08:15,940
And we can do the exact opposite
exaggeration, right?

195
00:08:15,940 --> 00:08:18,520
So you have the perfect evaluation, okay?

196
00:08:18,520 --> 00:08:22,060
If you have the perfect evaluation, okay,
then

197
00:08:22,060 --> 00:08:23,970
you will select the minimal number of
notes.

198
00:08:23,970 --> 00:08:25,690
Okay, so you see a little

199
00:08:25,690 --> 00:08:27,900
bit about what's best for branch and
bound.

200
00:08:27,900 --> 00:08:30,909
If the relaxation is really good, then,
the, the, what,

201
00:08:30,909 --> 00:08:33,623
the number that you explore would be
little, will be,

202
00:08:33,623 --> 00:08:36,396
you know, very small otherwise you, you
will explore a

203
00:08:36,396 --> 00:08:39,730
large number of notes, and these notes are
inside memory.

204
00:08:39,730 --> 00:08:40,420
Okay.

205
00:08:40,420 --> 00:08:43,020
Now, let me talk about the last search
technique that

206
00:08:43,020 --> 00:08:45,770
I want to talk about today, which is
limited discrepancy search.

207
00:08:45,770 --> 00:08:50,320
It's actually a very interesting and
amazing heur-, you know search strategy.

208
00:08:50,320 --> 00:08:50,930
And it assume

209
00:08:50,930 --> 00:08:52,260
that you have a good heuristic, okay.

210
00:08:52,260 --> 00:08:55,080
Assume that you have a greedy algorithm
which is really good, okay

211
00:08:55,080 --> 00:08:58,270
and that it makes very few mistakes okay,
that's what the issue.

212
00:08:58,270 --> 00:09:02,680
The heuristic is making very few mistakes
we assume that the search tree is

213
00:09:02,680 --> 00:09:05,182
binary, okay, and following that heuristic
mean,

214
00:09:05,182 --> 00:09:06,990
its always going left, left, left, left.

215
00:09:06,990 --> 00:09:09,440
Right, so you select item, you select the
item, you select the item.

216
00:09:09,440 --> 00:09:12,680
Okay, and branching right means the
heuristic is making a mistake.

217
00:09:12,680 --> 00:09:13,340
Okay.

218
00:09:13,340 --> 00:09:16,300
So if you do this, okay, this is what,

219
00:09:16,300 --> 00:09:18,490
you know, limited discrepancy search does,
okay.

220
00:09:18,490 --> 00:09:21,840
It wants to avoid mistake at all costs,
okay.

221
00:09:21,840 --> 00:09:26,430
So we basically explore a search space by
increasing number of mistakes.

222
00:09:26,430 --> 00:09:28,230
We first explore something which has, you

223
00:09:28,230 --> 00:09:30,470
know, zero mistakes, one mistakes, two
mistakes.

224
00:09:30,470 --> 00:09:34,692
Okay, and what we're doing there is really
trusting the heuristic lesson last.

225
00:09:34,692 --> 00:09:35,089
Okay.

226
00:09:35,089 --> 00:09:38,383
We start by trusting the heuristic, trying
to avoid mistakes, and then

227
00:09:38,383 --> 00:09:41,353
allow a certain number of mistakes and
then more and more mistakes

228
00:09:41,353 --> 00:09:43,840
until we don't trust the heuristic.
Okay.

229
00:09:43,840 --> 00:09:48,310
So in a sense the way to view this is that
limited discrepancy search can be viewed

230
00:09:48,310 --> 00:09:50,430
as a technique which is basically
exploring the

231
00:09:50,430 --> 00:09:53,940
search, this entire tree, through a bunch
of waves.

232
00:09:53,940 --> 00:09:56,540
The first waves assume that you make no
mistake.

233
00:09:56,540 --> 00:10:01,230
The next wave, you know, wave number one,
assume that you can make one mistake.

234
00:10:01,230 --> 00:10:04,590
And then wave number two, two mistakes,
and then so on and so forth.

235
00:10:04,590 --> 00:10:06,530
Okay?
So let me show you that on

236
00:10:06,530 --> 00:10:08,010
the exhaustive tree.
Okay?

237
00:10:08,010 --> 00:10:11,740
So you start at the root of use tree, and
then you assume that you make no mistake.

238
00:10:11,740 --> 00:10:13,240
You follow the heuristic all the time.

239
00:10:13,240 --> 00:10:13,860
That means what?

240
00:10:13,860 --> 00:10:16,700
Branching left, left, left, okay, and
that's what you see there.

241
00:10:16,700 --> 00:10:19,620
This is, you know, the first wave that you
see there.

242
00:10:19,620 --> 00:10:20,260
Okay.

243
00:10:20,260 --> 00:10:23,350
Now the second wave is going to lower you
form one mistake.

244
00:10:23,350 --> 00:10:24,050
Okay.

245
00:10:24,050 --> 00:10:27,880
So that basically means that you can only
branch once, on the right.

246
00:10:27,880 --> 00:10:28,500
Okay.

247
00:10:28,500 --> 00:10:31,414
So for instance you can go here, okay, but
then you would have to go left,

248
00:10:31,414 --> 00:10:32,420
left, left, left.

249
00:10:32,420 --> 00:10:34,680
Okay, and this is what you see in the
first wave.

250
00:10:34,680 --> 00:10:38,220
Okay, you go left, you go right and then
left, left, left, or you go left and

251
00:10:38,220 --> 00:10:42,610
then one right and then only left or left,
left and then one right over here, right.

252
00:10:42,610 --> 00:10:45,490
So this is what you explore in the second
wave,

253
00:10:45,490 --> 00:10:47,890
and there is something which is really
interesting here, right?

254
00:10:47,890 --> 00:10:52,670
So, this is half of the search space, this
is the second half of the search space.

255
00:10:52,670 --> 00:10:56,380
And what you can see here is already, in
wave, in the second wave,

256
00:10:56,380 --> 00:10:59,740
you are already exploring one particular
configuration, which

257
00:10:59,740 --> 00:11:02,290
is in the second half of the search space.

258
00:11:02,290 --> 00:11:05,290
Which depth first search would never do,
until, you know, it takes, it,

259
00:11:05,290 --> 00:11:10,270
it, it has explored the entire first half
of the search space, right?

260
00:11:10,270 --> 00:11:13,350
And this is one of the interesting things
about limited discrepancy search.

261
00:11:13,350 --> 00:11:17,110
It actually probes the search space in
many different places.

262
00:11:17,110 --> 00:11:17,820
Okay?

263
00:11:17,820 --> 00:11:19,560
Then the third wave is going to be makes

264
00:11:19,560 --> 00:11:21,510
two mistakes that makes, that means that
you can

265
00:11:21,510 --> 00:11:24,990
take two rights and one left.
Okay, and that's what you see there.

266
00:11:24,990 --> 00:11:27,335
And that point we have almost explored
everything.

267
00:11:27,335 --> 00:11:27,920
Okay.

268
00:11:27,920 --> 00:11:31,510
And then essentially the last web is
going to explore this guys on the left.

269
00:11:31,510 --> 00:11:35,910
Okay, so let me show you this example of
the knapsack example as well.

270
00:11:35,910 --> 00:11:36,700
Okay.

271
00:11:36,700 --> 00:11:40,850
So we start from the configuration, okay,
in wave one, we start from the root.

272
00:11:40,850 --> 00:11:43,070
And we branch left only, boom, boom, boom.

273
00:11:43,070 --> 00:11:44,650
And in this particular case, it's kind

274
00:11:44,650 --> 00:11:46,750
of annoying, because you get into
infeasibility

275
00:11:46,750 --> 00:11:48,150
very quickly.
Okay?

276
00:11:48,150 --> 00:11:51,060
The first wave, okay, is basically, you
can do one right.

277
00:11:51,060 --> 00:11:53,180
And this is what you get, okay?

278
00:11:53,180 --> 00:11:55,930
And, so you basically make one right
there, with, you,

279
00:11:55,930 --> 00:11:58,010
you make a mistake for, you know, refer
back to the

280
00:11:58,010 --> 00:12:00,680
heuristic, you let the heuristic make a
mistake, and then you

281
00:12:00,680 --> 00:12:04,100
go left, or you go right, there, and go
left again.

282
00:12:04,100 --> 00:12:06,930
And at that particular point, you find the
optimal solution already.

283
00:12:06,930 --> 00:12:07,640
Okay?

284
00:12:07,640 --> 00:12:10,660
And then in wave three, you explore two
mistakes for every one.

285
00:12:10,660 --> 00:12:11,850
And most of these things are going to be

286
00:12:11,850 --> 00:12:15,330
rejected, of course, because we have
already the optimal solution there, okay?

287
00:12:15,330 --> 00:12:17,100
So, limited discrepancy search.

288
00:12:17,100 --> 00:12:19,600
In summary, trust the greedy heuristic.

289
00:12:19,600 --> 00:12:22,420
When does it prune?
Think about it.

290
00:12:22,420 --> 00:12:23,690
When does it prune?

291
00:12:23,690 --> 00:12:25,870
Why, it's exactly like in that first
search.

292
00:12:25,870 --> 00:12:28,200
When you see a note, okay, it does an
evaluation

293
00:12:28,200 --> 00:12:31,040
which is worse than the best found
solution so far.

294
00:12:31,040 --> 00:12:32,550
You know that you don't have to explore

295
00:12:32,550 --> 00:12:35,170
it, so it prunes like in best-first
search.

296
00:12:35,170 --> 00:12:37,950
And no, here is an interesting question,
okay?

297
00:12:37,950 --> 00:12:40,710
Is it memory efficient?
Okay.

298
00:12:40,710 --> 00:12:45,310
So, compared to depth-first and best-first
search, is it memory efficient?

299
00:12:45,310 --> 00:12:49,090
Okay.
Now this is a very interesting question.

300
00:12:49,090 --> 00:12:50,070
You know why?

301
00:12:50,070 --> 00:12:53,100
Because I even told you really how to
implement this, okay.

302
00:12:53,100 --> 00:12:57,750
And depending upon the way you would
implement this search, okay.

303
00:12:57,750 --> 00:13:01,350
There will be an interesting trade off
between space and time, okay.

304
00:13:01,350 --> 00:13:03,230
You can implement it very efficiently,

305
00:13:03,230 --> 00:13:05,000
but it will take a lot of space.

306
00:13:05,000 --> 00:13:08,890
Or you can implement it by doing redundant
work and it will take minimal space.

307
00:13:08,890 --> 00:13:10,502
So the implementation can be between

308
00:13:10,502 --> 00:13:13,240
the best-first and the depth-first search
approach.

309
00:13:13,240 --> 00:13:13,810
Okay?

310
00:13:13,810 --> 00:13:15,930
So think about it and think how you would
do this.

311
00:13:15,930 --> 00:13:17,682
This is a very interesting topic.

312
00:13:17,682 --> 00:13:20,770
Now limited discrepancy searches on
beautiful application is scheduling.

313
00:13:20,770 --> 00:13:22,395
We'll come back to that and rounding as
well.

314
00:13:22,395 --> 00:13:24,800
So it's a very interesting technique when
you have

315
00:13:24,800 --> 00:13:26,990
a good heuristic, when you know what
you're doing.

316
00:13:26,990 --> 00:13:28,140
Okay?
So,

317
00:13:28,140 --> 00:13:29,270
let me conclude here.

318
00:13:29,270 --> 00:13:31,520
So, we have seen search strategies here

319
00:13:31,520 --> 00:13:33,490
and we have two class time about
relaxation.

320
00:13:33,490 --> 00:13:34,060
Okay?

321
00:13:34,060 --> 00:13:38,530
The big idea last time was relaxation.
The big idea today is search, okay.

322
00:13:38,530 --> 00:13:40,670
Now how do you choose between them, okay.

323
00:13:40,670 --> 00:13:42,870
This is a very interesting topic in
optimization.

324
00:13:42,870 --> 00:13:45,120
This is one of the most important topic.

325
00:13:45,120 --> 00:13:48,750
You can find really strong relaxation, but
they will be so slow that

326
00:13:48,750 --> 00:13:51,430
you will explore a very small tree, but it
take a long time.

327
00:13:51,430 --> 00:13:54,150
Or you can have a relaxation, which is
really strong.

328
00:13:54,150 --> 00:13:56,060
The trees and a really fast computer.

329
00:13:56,060 --> 00:13:58,980
And, the trees the size of your search
base substantially.

330
00:13:58,980 --> 00:14:01,470
So, you have to find, you know the right
relaxation and

331
00:14:01,470 --> 00:14:04,020
the right part of the search tree that you
will explore.

332
00:14:04,020 --> 00:14:06,260
This is a very interesting topic.
Okay?

333
00:14:06,260 --> 00:14:09,620
Most of this is going to be problem
specific like everything in optimization.

334
00:14:09,620 --> 00:14:10,290
Okay?

335
00:14:10,290 --> 00:14:12,580
And finally, you know, there are many
other strategies.

336
00:14:12,580 --> 00:14:16,390
And, you know, I really encourage you to
actually think outside the box, and

337
00:14:16,390 --> 00:14:19,550
think, if you can find all the kind of
strategies that are going to be good.

338
00:14:19,550 --> 00:14:21,490
Okay?
Think about it, okay?

339
00:14:21,490 --> 00:14:23,620
So, at this point, this is very
interesting.

340
00:14:23,620 --> 00:14:27,050
We have covered, you know the knapsack
problem entirely.

341
00:14:27,050 --> 00:14:27,610
Okay.

342
00:14:27,610 --> 00:14:30,530
It's time for you to start the
assignments, okay, and we'll

343
00:14:30,530 --> 00:14:34,100
go into more sophisticated techniques in
the next couple of lectures.

344
00:14:34,100 --> 00:14:35,370
Thank you very much guys.

345
00:14:35,370 --> 00:14:39,780
Okay, so it's time to actually go to work
and actually solve these assignments.

346
00:14:39,780 --> 00:14:39,950
Bye.

